{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1bc453d-c8d3-4503-b3da-52120ad92c74",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Reflection Pattern\n",
    "\n",
    "The first pattern we are going to implement is the **reflection pattern**. \n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"./images/reflection.png\" alt=\"Alt text\" width=\"600\"/>\n",
    "\n",
    "---\n",
    "\n",
    "This pattern allows the LLM to reflect and critique its outputs, following the next steps:\n",
    "\n",
    "1. The LLM **generates** a candidate output. If you look at the diagram above, it happens inside the **\"Generate\"** box.\n",
    "2. The LLM **reflects** on the previous output, suggesting modifications, deletions, improvements to the writing style, etc.\n",
    "3. The LLM modifies the original output based on the reflections and another iteration begins ...\n",
    "\n",
    "**Now, we are going to build, from scratch, each step, so that you can truly understand how this pattern works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7898c34d-de9a-4970-b7f4-3d86b69d45a7",
   "metadata": {},
   "source": [
    "## Generation Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f4d7b7-40bf-43b9-a626-2a11d5529ac8",
   "metadata": {},
   "source": [
    "### Groq Client and relevant imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96731d2f-a079-4e41-9756-220f02d4ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import display_markdown\n",
    "\n",
    "# Remember to load the environment variables. You should have the Groq API Key in there :)\n",
    "load_dotenv()\n",
    "\n",
    "client = Groq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d91a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_client(llm_choice):\n",
    "    if llm_choice == \"GROQ\":\n",
    "        client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "        return client\n",
    "    elif llm_choice == \"OPENAI\":\n",
    "        load_dotenv()  # load environment variables from .env fil\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        return client\n",
    "    else:\n",
    "        raise ValueError(\"Invalid LLM choice. Please choose 'GROQ' or 'OPENAI'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a126526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY exists and begins sk-proj-1WUVgv...\n",
      "GROQ_API_KEY exists and begins gsk_11hFN1EMfj...\n",
      "LLM_CHOICE: GROQ - MODEL: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# LLM_CHOICE = \"OPENAI\"\n",
    "LLM_CHOICE = \"GROQ\"\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"OPENAI_API_KEY exists and begins {OPENAI_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    print(f\"GROQ_API_KEY exists and begins {GROQ_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"GROQ_API_KEY not set\")\n",
    "\n",
    "\n",
    "client = get_llm_client(LLM_CHOICE)\n",
    "if LLM_CHOICE == \"GROQ\":\n",
    "    MODEL = \"llama-3.3-70b-versatile\"\n",
    "else:\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"LLM_CHOICE: {LLM_CHOICE} - MODEL: {MODEL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e644a635-e035-44e2-8c25-cee0f2b56556",
   "metadata": {},
   "source": [
    "We will start the **\"generation\"** chat history with the system prompt, as we said before. In this case, let the LLM act like a Python \n",
    "programmer eager to receive feedback / critique by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12467256-c741-495a-9923-439c1fcf270d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a Python programmer tasked with generating high quality Python code.\"\n",
    "        \"Your task is to Generate the best content possible for the user's request. If the user provides critique,\" \n",
    "        \"respond with a revised version of your previous attempt.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43149b4f-54db-455f-9d39-6ad2f5c52b94",
   "metadata": {},
   "source": [
    "Now, as the user, we are going to ask the LLM to generate an implementation of the **Merge Sort** algorithm. Just add a new message with the **user** role to the chat history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0742e7bd-4857-4ed1-a96b-37098d448bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Create an AI Agent that does reflection.\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1bffe-375f-4a9a-8433-e217eb94aea2",
   "metadata": {},
   "source": [
    "Let's generate the first version of the essay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff984277-733c-4495-b7fd-0669393380b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_code = client.chat.completions.create(\n",
    "    messages=generation_chat_history,\n",
    "    model=\"llama3-70b-8192\"\n",
    ").choices[0].message.content\n",
    "\n",
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": generated_code\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c03f208b-2234-4fd1-a02b-f4fff06c01a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's an example of a basic AI agent that performs reflection in Python:\n",
       "```\n",
       "import inspect\n",
       "import types\n",
       "\n",
       "class ReflectionAI:\n",
       "    def __init__(self):\n",
       "        self.knowledge_base = {}\n",
       "\n",
       "    def learn(self, func):\n",
       "        \"\"\"\n",
       "        Learn a new function and store its metadata in the knowledge base.\n",
       "        \"\"\"\n",
       "        self.knowledge_base[func.__name__] = {\n",
       "            'docstring': func.__doc__,\n",
       "            'args': inspect.getfullargspec(func).args,\n",
       "            'source_code': inspect.getsource(func)\n",
       "        }\n",
       "\n",
       "    def reflect(self, func_name):\n",
       "        \"\"\"\n",
       "        Reflect on a learned function and return its metadata.\n",
       "        \"\"\"\n",
       "        if func_name in self.knowledge_base:\n",
       "            return self.knowledge_base[func_name]\n",
       "        else:\n",
       "            return None\n",
       "\n",
       "    def introspect(self):\n",
       "        \"\"\"\n",
       "        Return a list of all learned functions and their metadata.\n",
       "        \"\"\"\n",
       "        return self.knowledge_base\n",
       "\n",
       "    def think(self):\n",
       "        \"\"\"\n",
       "        Perform some thinking/reflection on the learned functions.\n",
       "        \"\"\"\n",
       "        # For now, just print out the introspection results\n",
       "        print(\"Thinking...\")\n",
       "        print(self.introspect())\n",
       "\n",
       "def add(a, b):\n",
       "    \"\"\"Adds two numbers.\"\"\"\n",
       "    return a + b\n",
       "\n",
       "def greeting(name):\n",
       "    \"\"\"Says hello to someone.\"\"\"\n",
       "    print(f\"Hello, {name}!\")\n",
       "\n",
       "ai = ReflectionAI()\n",
       "ai.learn(add)\n",
       "ai.learn(greeting)\n",
       "\n",
       "print(ai.reflect('add'))\n",
       "# Output: {'docstring': 'Adds two numbers.', 'args': ['a', 'b'], 'source_code': 'def add(a, b):\\n    \"\"\"Adds two numbers.\"\"\"\\n    return a + b\\n'}\n",
       "\n",
       "ai.think()\n",
       "# Output: {'add': {'docstring': 'Adds two numbers.', 'args': ['a', 'b'], 'source_code': 'def add(a, b):\\n    \"\"\"Adds two numbers.\"\"\"\\n    return a + b\\n'}, 'greeting': {'docstring': 'Says hello to someone.', 'args': ['name'], 'source_code': 'def greeting(name):\\n    \"\"\"Says hello to someone.\"\"\"\\n    print(f\"Hello, {name}!\")\\n'}}\n",
       "```\n",
       "This AI agent has four main methods:\n",
       "\n",
       "1. `learn(func)`: Learns a new function and stores its metadata (docstring, arguments, and source code) in the knowledge base.\n",
       "2. `reflect(func_name)`: Reflects on a learned function and returns its metadata.\n",
       "3. `introspect()`: Returns a list of all learned functions and their metadata.\n",
       "4. `think()`: Performs some thinking/reflection on the learned functions (currently just prints out the introspection results).\n",
       "\n",
       "The agent learns two sample functions, `add` and `greeting`, and then performs reflection and introspection on them.\n",
       "\n",
       "Please let me know if this meets your expectations or if you'd like me to revise anything!"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(generated_code, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a04ebe5-0573-4520-a529-aff22d486b7d",
   "metadata": {},
   "source": [
    "## Reflection Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aa69e4-632f-4a0c-a6f0-c5a7ced4849d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now, let's allow the LLM to reflect on its outputs by defining another system prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d93c928-d585-48af-a74c-a5b8d84593c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reflection_chat_history = [\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are an experienced Python code reviwer. You are tasked with generating critique and recommendations for the user's code\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c498175f-b3f9-40af-92a3-d5b36d77d1cf",
   "metadata": {},
   "source": [
    "The user message, in this case,  is the essay generated in the previous step. We simply add the `generated_codee` to the `reflection_chat_history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26af1a73-4d91-40e8-a9bc-c34d32b2ab82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reflection_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": generated_code\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa994c8-3612-47b0-9571-e21d0d73d896",
   "metadata": {},
   "source": [
    "Now, let's generate a critique to the Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40fee42f-d47a-41b1-a40d-7208ba76ce98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "critique = client.chat.completions.create(\n",
    "    model=MODEL, messages=generation_chat_history, temperature=0.7\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fef3203-c7f1-407f-8b9b-4e8ae140a4cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "\n",
       "Note: The above code uses Python's built-in `inspect` module to extract metadata from the learned functions. The `inspect` module provides several useful functions for inspecting live objects such as modules, classes, objects, etc. \n",
       "\n",
       "### Advanced Usage\n",
       "\n",
       "To make this agent more advanced, you could add the following features:\n",
       "\n",
       "*   **Learning from other sources**: Currently, the agent only learns from functions passed directly to it. You could modify it to learn from other sources, such as files, databases, or even other AI agents.\n",
       "*   **More advanced thinking/reflection**: The `think()` method currently just prints out the introspection results. You could modify it to perform more complex thinking/reflection tasks, such as analyzing the relationships between learned functions or generating new functions based on the learned knowledge.\n",
       "*   **Error handling and validation**: The agent currently assumes that all learned functions are valid and can be introspected successfully. You could add error handling and validation to handle cases where this is not true.\n",
       "*   **Improved knowledge representation**: The agent currently stores learned functions in a simple dictionary. You could modify it to use a more advanced knowledge representation, such as a graph or ontology, to store and reason about the learned knowledge.\n",
       "\n",
       "### Example Use Cases\n",
       "\n",
       "Here are some example use cases for this AI agent:\n",
       "\n",
       "*   **Automated programming**: The agent could be used to automate the process of learning and reflecting on code. For example, it could be used to generate documentation for a large codebase or to identify patterns and relationships between different parts of the code.\n",
       "*   **Code analysis and optimization**: The agent could be used to analyze and optimize code. For example, it could be used to identify performance bottlenecks or to suggest improvements to the code.\n",
       "*   **Education and training**: The agent could be used to educate and train developers. For example, it could be used to generate interactive coding exercises or to provide personalized feedback and guidance to developers.\n",
       "\n",
       "Please let me know if you would like me to revise or add anything to the above code. I am here to help."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(critique, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df433b0-d662-4378-895e-6b09dd3201bc",
   "metadata": {},
   "source": [
    "Finally, we just need to add this *critique* to the `generation_chat_history`, in this case, as the `user` role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27a85bb3-cf6a-4576-8caf-cd41e602a1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generation_chat_history.append(\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": critique\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c1aefa-8454-41ab-af40-2675f340a577",
   "metadata": {},
   "source": [
    "## Generation Step (II)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "91d845cf-51c3-4cfd-b6a7-1b970413f6db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "essay = client.chat.completions.create(\n",
    "    model=MODEL, messages=generation_chat_history, temperature=0.7\n",
    ").choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ef14eaa8-f501-4efc-997f-8564ec8dccd8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       " \n",
       "\n",
       "### How does this code work?\n",
       "1. **Initialization**: An empty `knowledge_base` dictionary is created to store metadata about learned functions.\n",
       "2. **Learning**: The `learn` function takes another function as input and extracts its metadata using the `inspect` module. This metadata is then stored in the `knowledge_base`.\n",
       "3. **Reflection**: The `reflect` function takes a function name as input and returns the corresponding metadata from the `knowledge_base`.\n",
       "4. **Introspection**: The `introspect` function returns the entire `knowledge_base`, which contains metadata for all learned functions.\n",
       "5. **Thinking**: The `think` function currently just prints out the results of introspection, but this is where you could implement more complex thinking/reflection logic.\n",
       "\n",
       "### Possible improvements\n",
       "* **Add more thinking/reflection logic**: Currently, the `think` function just prints out the introspection results. You could add more complex logic here to analyze the learned functions and their metadata.\n",
       "* **Support for more advanced function metadata**: The current implementation only extracts basic metadata like docstrings and argument names. You could add support for more advanced metadata, such as function return types or docstring parsing.\n",
       "* **Error handling**: The current implementation assumes that the input functions are valid and can be learned. You could add error handling to handle cases where the input functions are invalid or cannot be learned.\n",
       "* **Support for other types of knowledge**: The current implementation only supports learning functions. You could add support for learning other types of knowledge, such as data structures or classes."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_markdown(essay, raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75883af2-f31d-4c24-b1ff-315a0711f9fa",
   "metadata": {},
   "source": [
    "## And the iteration starts again ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
