{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8e520df",
   "metadata": {},
   "source": [
    "<img src=\"./images/03-faq.png\" width=700px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44f832a",
   "metadata": {},
   "source": [
    "In this example we will see an example of RAG - Retrieval Augmented Generation.\n",
    "\n",
    "We have 'retrieved' some data and stored in the FAQ variable. In this demo, this is hard coded but there are a multitude of ways of RETRIEVING data to AUGMENT the prompt and consequent GENERATION.\n",
    "\n",
    "OpenAi does not have any domain specifice knowledge. In lieu of a fine tuned model on our domain, we can AUGMENT the prompt with retrieved data.\n",
    "\n",
    "RAG is not just asking questions of documents using vector search - it is augmenting the LLM request with additonal data in lieu of fine tuning the model.\n",
    "\n",
    "We are going to give it a list of FAQs so that we can use a ChatBot to ask questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70e39cd8-ec79-4e3e-9c26-5659d42d0861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mrcra\\Desktop\\DJCON_EU\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16b03111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_client(llm_choice):\n",
    "    if llm_choice == \"GROQ\":\n",
    "        client = OpenAI(\n",
    "            base_url=\"https://api.groq.com/openai/v1\",\n",
    "            api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    "        )\n",
    "        return client\n",
    "    elif llm_choice == \"OPENAI\":\n",
    "        load_dotenv()  # load environment variables from .env fil\n",
    "        client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        return client\n",
    "    else:\n",
    "        raise ValueError(\"Invalid LLM choice. Please choose 'GROQ' or 'OPENAI'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "231605aa-fccb-447e-89cf-8b187444536a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY exists and begins sk-proj-1WUVgv...\n",
      "GROQ_API_KEY exists and begins gsk_11hFN1EMfj...\n",
      "LLM_CHOICE: GROQ - MODEL: llama-3.3-70b-versatile\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "LLM_CHOICE = \"OPENAI\"\n",
    "LLM_CHOICE = \"GROQ\"\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    print(f\"OPENAI_API_KEY exists and begins {OPENAI_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"OPENAI_API_KEY not set\")\n",
    "\n",
    "if GROQ_API_KEY:\n",
    "    print(f\"GROQ_API_KEY exists and begins {GROQ_API_KEY[:14]}...\")\n",
    "else:\n",
    "    print(\"GROQ_API_KEY not set\")\n",
    "\n",
    "\n",
    "client = get_llm_client(LLM_CHOICE)\n",
    "if LLM_CHOICE == \"GROQ\":\n",
    "    MODEL = \"llama-3.3-70b-versatile\"\n",
    "else:\n",
    "    MODEL = \"gpt-4o-mini\"\n",
    "\n",
    "print(f\"LLM_CHOICE: {LLM_CHOICE} - MODEL: {MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1eacc8a4-4b48-4358-9e06-ce0020041bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A request to the LLM is stateless so we will always need to pass all the data that is needed each time.\n",
    "\n",
    "# `history` is just that - a record of what has gone on before so that the LLM can have context to answer the query.\n",
    "\n",
    "\n",
    "# We will use GRADIO as our UI.\n",
    "def chat(message, history):\n",
    "    # history is part of the gradio ChatInterface and it stores previous answers\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_message}]\n",
    "        # + history ## groq gives error as it adds metadata\n",
    "        + [{\"role\": \"user\", \"content\": message}]\n",
    "    )\n",
    "    print(\"History is:\")\n",
    "    print(history)\n",
    "    print(\"And messages is:\")\n",
    "    print(messages)\n",
    "    # ====================\n",
    "    # AI bit\n",
    "    stream = client.chat.completions.create(model=MODEL, messages=messages, stream=True)\n",
    "\n",
    "    # Just UI implementation\n",
    "    response = \"\"\n",
    "    for stream_so_far in stream:\n",
    "        response += stream_so_far.choices[0].delta.content or \"\"\n",
    "        yield response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f91b414-8bab-472d-b9c9-3fa51259bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"\"\"\n",
    "You are a report selection agent. \n",
    "You are very good at returning the best report to answer a user's question. \n",
    "For example, if a user wants a joke, you reply with |TOOL: **get_joke**|\n",
    "Another example, if they want a **TOTAL SALES** report then |TOOL: **get_sales**| would be the best report.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d57e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** just for bold in front end but does add some importance too.\n",
    "\n",
    "REPORTS = [\n",
    "    \"For weather use this tool => **get_weather**.\",\n",
    "    \"For hotel booking use this tool => **get_hotel_booking**.\",\n",
    "    \"For car hire use this tool => **get_car_hire**.\",\n",
    "    \"For flight booking use this tool => **get_flight**.\",\n",
    "    \"For total sales use this tool => **get_sales**.\",\n",
    "    \"For site statistics use this tool => **get_site_statistics**.\",\n",
    "    \"For jokes use this tool => **get_joke**.\",\n",
    "    \"To add two numbers use this tool => **get_adder**.\",\n",
    "    \"To compute loss function => **get_loss**.\",\n",
    "    \"To critique an article prior to publication but not after publication use this tool => **get_article_review**.\",\n",
    "    \"For all other requests use this tool => **customer_service_agent**.\",\n",
    "]\n",
    "\n",
    "# Again, 'character and instruction' along with knowledge\n",
    "system_message += \"\\n\" + \"\\n\".join(REPORTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4f1b89",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "413e9e4e-7836-43ac-a0c3-e1ab5ed6b136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History is:\n",
      "[]\n",
      "And messages is:\n",
      "[{'role': 'system', 'content': \"\\nYou are a report selection agent. \\nYou are very good at returning the best report to answer a user's question. \\nFor example, if a user wants a joke, you reply with |TOOL: **get_joke**|\\nAnother example, if they want a **TOTAL SALES** report then |TOOL: **get_sales**| would be the best report.\\n\\nFor weather use this tool => **get_weather**.\\nFor hotel booking use this tool => **get_hotel_booking**.\\nFor car hire use this tool => **get_car_hire**.\\nFor flight booking use this tool => **get_flight**.\\nFor total sales use this tool => **get_sales**.\\nFor site statistics use this tool => **get_site_statistics**.\\nFor jokes use this tool => **get_joke**.\\nTo add two numbers use this tool => **get_adder**.\\nTo compute loss function => **get_loss**.\\nTo critique an article prior to publication but not after publication use this tool => **get_article_review**.\\nFor all other requests use this tool => **customer_service_agent**.\"}, {'role': 'user', 'content': 'flight to rome'}]\n"
     ]
    }
   ],
   "source": [
    "# We use Gradio for a chat interface\n",
    "# prompt: I am want a plane to Rome then an auto to Paris\n",
    "\n",
    "gr.ChatInterface(fn=chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
